<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title> SPAD: Spatially Aware Multiview Diffusers </title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--FACEBOOK-->


    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>

</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b><em>SPAD</em></b> : Spatially Aware Multiview Diffusers<br>
                <!-- <small>
                    Accepted to SIGGRAPH Asia, 2023
                </small> -->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://yashkant.github.io/">
                            Yash Kant
                        </a>
                        <br>UofToronto & Vector 
                    </li>
                    <li>
                        <a href="https://wuziyi616.github.io/">
                            Ziyi Wu
                        </a>
                        <br>UofToronto & Vector 
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=LXBfhBcAAAAJ&hl=en">
                            Michael Vasilkovsky
                        </a>
                        <br>Snap Research
                    </li>
                    <li>
                        <a href="https://guochengqian.github.io/">
                            Guocheng Qian
                        </a>
                        <br>KAUST
                    </li>
                    <li>
                        <a href="https://alanspike.github.io/">
                            Jian Ren
                        </a>
                        <br>Snap Research
                    </li>
                    <br>
                    <li>
                        <a href="http://alpguler.com/">
                            Riza Alp Guler
                        </a>
                        <br>Snap Research
                    </li>
                    <li>
                        <a href="https://www.bernardghanem.com/">
                            Bernard Ghanem
                        </a>
                        <br>KAUST
                    </li>
                    <li>
                        <a href="http://www.stulyakov.com/">
                            Sergey Tulyakov*
                        </a>
                        <br>Snap Research
                    </li>
                    <li>
                        <a href="https://www.gilitschenski.org/igor/">
                            Igor Gilitschenski*
                        </a>
                        <br>UofToronto & Vector 
                    </li>
                    <li>
                        <a href="https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/">
                            Aliaksandr Siarohin*
                        </a>
                        <br>Snap Research
                    </li>
                    <br>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="spad.pdf">
                            <image src="img/spad-ss.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="">
                            <image src="img/supp_img.png" height="60px">
                                <h4><strong>Supplementary</strong></h4>
                        </a>
                    </li> -->
                    <!-- <li>
                        <a href="img/vids/supp_video.mp4">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Supplementary Video</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="spad-ppt.pdf">
                            <image src="img/ppt.svg" height="60px">
                                <h4><strong>Slides</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                            <a href="https://github.com/">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                </ul>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">

                <video muted="" autoplay="" loop="" width="100%" height="100%">
                    <source src="img/vids/TODO" type="video/mp4">
                </video>
    
            </div>
        </div> -->



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    tl;dr
                </h3>
                <image src="img/text-spad.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>
        <hr>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@misc{kant2024spad,
      title={SPAD : Spatially Aware Multiview Diffusers}, 
      author={Yash Kant and Ziyi Wu and Michael Vasilkovsky and Guocheng Qian and Jian Ren and Riza Alp Guler and Bernard Ghanem and Sergey Tulyakov and Igor Gilitschenski and Aliaksandr Siarohin},
      year={2024},
      eprint={2402.05235},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</textarea>
                </div>
            </div>
        </div>
        <hr>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview: <b><em>SPAD</em></b>
                </h3>
                <!-- <p class="text-justify">
                    <ul>
                        <li>
                            We present a method for generating consistent multi-view images from a text prompt or a single input image, which fine-tunes a large-scale pretrained 2D diffusion model on rendered images of 3D objects. 
                        </li>
                        <li>
                            To enable multi-view generation, we extend the self-attention layers of the diffusion model with cross-view interactions to synchronize information between views.
                        </li>
                        <li>
                            We further incorporate the Epipolar Attention and Pl√ºcker coordinate embeddings to enhance the 3D consistency of our generated multi-view images.
                        </li>
                    </ul>
                </p> -->
                <br>
                <!-- # TODO: use video here? -->
                <!-- <div class="text-center">
                    <div style="text-align:center;">
                        <image src="img/assets/teaser.png" class="img-responsive" alt="teaser"></image>
                    </div>
                </div> -->
                <video muted="" autoplay="" loop="" width="100%" height="100%">
                    <source src="img/vids/teaser.mp4" type="video/mp4">
                </video>
                <br>
                Given a text prompt, our method synthesizes 3D consistent views of the same object. Our model can generate many images from arbitrary camera viewpoints, while being trained only on four views. <b>Here, we show eight views sampled uniformly at a fixed elevation generated in a <u>single forward pass.</u></b>
            </div>
        </div>
        <hr>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method: <b> <em>SPAD</em> </b>
                </h3>
                <div class="text-center">
                    <div style="text-align:center;">
                        <image src="img/assets/model-pipeline.png" class="img-responsive" alt="pipeline"></image>
                    </div>
                </div>
                <br>
                <p class="text-justify">
                    <b>Model pipeline.</b>
                    (a) We fine-tune a pre-trained text-to-image diffusion model on multi-view rendering of 3D objects.
                    <br>
                    (b) Our model jointly denoises noisy multi-view images conditioned on text and relative camera poses.
                    To enable cross-view interaction, we apply 3D self-attention by concatenating all views, and enforce epipolar constraints on the attention map.
                    <br>
                    (c) We further add Pl√ºcker Embedding to the attention layers as positional encodings, to enhance camera control.
                </p>

                <br>
                <div class="text-center">
                    <div style="text-align:center;">
                        <image src="img/assets/sub-modules.png" class="img-responsive" alt="sub-modules"></image>
                    </div>
                </div>
                <p class="text-justify">
                    <b>Epipolar Attention (Left).</b>
                    For each <font color="red">source point S</font> on a feature map, we compute its <font color="red">epipolar lines</font> on all other views.
                    <font color="red">S</font> will only attend to features along these lines plus all the points on itself (<font color="DodgerBlue">blue points</font>).
                    <br>
                    <b>Illustration of one block in SPAD (Right).</b>
                    We add Pl√ºcker Embedding to feature maps in the self-attention layer by inflating the original QKV projection layers with zero projections.
                </p>


            </div>
        </div>
        <br>
        <hr>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h3>
                Text-to-3D Generation: <b>Multi-view Triplane Generator</b>
            </h3>
            <p style="text-align:center;">
                <image src="img/vids/triplane2.gif" class="img-responsive" alt="3D-Triplane-NeRF">
            </p>
            <p style="text-align:center;">
                <image src="img/vids/triplane1.gif" class="img-responsive" alt="3D-Triplane-NeRF">
            </p>
            Similar to <a href="https://instant-3d.github.io/"> Instant3D</a>, we train a multi-view to NeRF generator. We use four multi-view generations from SPAD (shown in bottom half) as input to generator and create 3D assets. <b>The entire generation takes ~ 10 seconds.</b> 
            
            <br>

        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h3>
                Text-to-3D Generation: <b>Multiview SDS</b>
            </h3>
            <!-- <p style="text-align:center;">
                <image src="img/assets/text-to-3d.png" class="img-responsive" alt="3D-SDS">
            </p> -->
            <video muted="" autoplay="" loop="" width="100%">
                <source src="img/vids/sds.mp4" type="video/mp4">
            </video>
    
            Thanks to our 3D consistent multi-view generation, we can leverage the multi-view Score Distillation Sampling (SDS) for 3D asset generation.
            We integrate SPAD into <a href="https://github.com/threestudio-project/threestudio">threestudio</a> and follow the training setting of <a href="https://mv-dream.github.io/">MVDream</a> to train a NeRF.
            <br>
    
        </div>
    
    
    
    
    </div>
    <br>
    <hr>
    <!-- <div class="row">
        <div class="col-md-8 col-md-offset-2">
        <h3>
            Qualitative Result: <b>More Text-Guided Multi-View Generation Results</b>
        </h3>
        Here, we show more multi-view generation results of SPAD.
        <br><br>
        <p style="text-align:center;">
            <image src="img/assets/more-text-to-mv.png" class="img-responsive" alt="text-to-mv">
        </p>
        </div>
    </div> -->

            <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Quantitative Result: <b>Novel View Synthesis</b>
                </h3>
                To evaluate the 3D consistency of our method, we adapt SPAD to the image-conditioned novel view synthesis task.
                <!-- We simply re-train a model which replaces the text embedding input with the CLIP image feature of the source image. -->
                We test on unseen 1,000 Objaverse objects, and all objects from the Google Scanned Objects (GSO) dataset.
                <br><br>
                <p style="text-align:center;">
                    <image src="img/assets/nvs-quant-results.png" class="img-responsive" alt="NVS">
                </p>
                <b>SPAD preserves structural and perceptual details faithfully.</b>
                Our method achieves competitive results on PSNR and SSIM, which setting new state-of-the-art on LPIPS.
            </div>
        </div>
        <br>
        <hr>

        <div class="row">

            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Qualitative Result: <b>Comparison with MVDream</b>
                </h3>
                <p style="text-align:center;">
                    <image src="img/assets/compare.jpg" class="img-responsive" alt="compare">
                </p>
            </div>

            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Qualitative Result: <b>Close View Generation</b>
                </h3>
                We demonstrate smooth transition between views by generate close viewpoints each varying by 10-degrees along azimuth.

                <br><br>
                <p style="text-align:center;">
                    <image src="img/assets/close_view_10deg.jpg" class="img-responsive" alt="close_view_10deg">
                </p>
            </div>

            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Qualitative Result: <b>Ablation Study</b>
                </h3>
                We ablate our method on various design choices to demonstrate their importance.
                <br><br>
                <p style="text-align:center;">
                    <image src="img/assets/ablation.jpg" class="img-responsive" alt="ablation">
                </p>
                <b>Epipolar Attention promotes better camera control in SPAD.</b>
                Directly applying 3D self-attention on all the views leads to content copying between generated images, as highlighted by the <font color="DodgerBlue">blue circles</font>.
                <br>
                <b>Pl√ºcker Embeddings help prevent generation of flipped views.</b>
                Without Pl√ºcker Embeddings, the model sometimes predict image regions that are rotated by 180¬∞, as highlighted by the <font color="red">red circles</font>, due to the ambiguity in epipolar lines.
            </div>


        </div>
        <br>
        <hr>


    <!-- <br>
    <hr>
    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Qualitative Result: <b>Failure Modes</b>
            </h3>
            We find that <b> <em> iNVS </em> </b> struggles most when monocular depth estimator generates inaccurate depth</em>.
            <br>
            <p style="text-align:center;">
                <image src="img/assets/failuresv2.png" height="50px" class="img-responsive">
            </p>
        </div>
    </div> -->
    <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p>
                    The website template was borrowed from <a href="http://mgharbi.com/">Micha√´l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>

</html>
